1.
Purch line import. PurchPurchaseOrderLineV2Entity - more than 1000 lines
This entity implements a differentiation between a field(as a column name) not specified in the template and a field that is specified, but the value is empty. E.g. if the price column is specified, price will by taken from this column. So, for example, if you want to support    Integration where the price may be taken from the D365FO price agreement or specified during the import, you need to pass 2 different data packages(in different formats). There is no description on this, the only way to find how this works - spend a time to debugging the entity logic. 

How this is solved in external integration:
Integration logic will be coded in X++ as a part of the initial design(e.g. what to do if the Price is empty), External system will use one data message format(template)  and don't need to know about processing rules on D365FO side

2.
PurchPurchaseOrderLineV2Entity - users reported integration error, as a test case - they populate fields used in the integration message on the PO form in specified sequence and it produces different results than using the entity during the integaraion. 
Standard solution: a lot of debugging with many attempts, trying to match the form behaviour, where one fix breaking another scenario. finally some post update events 

External integration solution: just change the purchase line creation code(with modifiedField methods call) match to the required sequence
  

3.
There is a requirement to send updated lines during the PO confirmation process.
External integration solution: There is an incremental template for this.  create an event for Purchase order, that is triggered on PO confirmation
During the message processing(on X++ side, that may not be in sync with the current confirmation), analyse all confirmation rows and mark those that have already been sent with a flag. 

Standard solution: PurchPurchaseOrderConfirmationLineEntity standard entity is a use of UNION purchase lines and history lines. There is no option to read only modified lines, you need to repeat all logic related to union query to create staging tables on the integration side, perform multiple reads from D365FO
 
External teams wants to implement a incremental 



2 hours ago

I have a concerning issue with occasionally failing data management imports that are triggered from an integration.

 

The integration uses OData actions available on the DataManagementDefinitionGroups entity. It first calls GetAzureWriteUrl to obtain a link to Azure storage. Using this link the file is uploaded to Azure storage. Finally ImportFromPackage is called to schedule a batch job to execute the import. This method has been in use successfully for around 3 years since go-live. 

 

Recently we noticed rare occasions where the import does not succeed, and there is no record of the import in the job history section of the data management workspace. The missing history is caused by the DMFExecution table not having a record related to the import, all other tables such as DMFDefinitionGroupExecution and the related staging tables have records for the executions that are missing from DMFExecution.

 

After enabling database logging we found that the DMFExecution record was deleted during the DMF import batch execution, but not by the integration service user account, or by any other user in the system. Corelating the time of deletion with the DMF staging execution history cleanup log we suspected this system job to be the cause.

 

We have raised the issue with support and received the below information.

"We can confirm that there is a system job (not the DMF cleanup batch job) that performs the clean up with (-1) values

 

This script (system job) is to ensure that all staging tables stay at a manageable size. It's distinct from the DMF staging cleanup that runs in batch.

 

This clean up runs several times a day and its recurrence is controlled by Microsoft. This is in your case is set up to run every 6 hours for example.

 

This job cleans mainly the orphaned or stale staging records

 

A stale record is normally the data that is outdated or no longer relevant. This can happen when a data import/export job was interrupted or failed. For example: A staging table still contains data from a previous import that was never finalized.

An orphaned record is a record that no longer has a valid link to its parent or related data. This usually happens when a related record (like a header or parent entity) was deleted or a process removed a key reference, but didnâ€™t clean up all dependent records. For example: A line item in a staging table exists, but its corresponding header record was deleted. The line item is now "orphaned" because it has no parent.

This system job cannot be put on hold because those stale or orphaned records might cause some issues until the original DMF cleanup batch job is run again based on the recurrence customers set (7 days I believe in your case), so deleting them earlier is intended."

 

The script mentioned is DmfOrphanCleanup_CleanupStaleStagingRecords.sql

 

The explanation does not help us to solve our issue, since the records deleted are not stale in our opinion, they are active imports.

 

We have been advised by support that they cannot provide any further assistance and that we should address our concerns to the product team using this forum.

 

SR#2504221420000023